{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=100\n",
      "CHECKPOINT_EVERY=50\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=50\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=10\n",
      "NUM_FILTERS=100\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 100, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 50, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 50, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "# Build vocabulary -> learn library preprocessing.VocabularyProcessor를 이용해서 \n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    def __init__(self,\n",
    "                sequence_length,\n",
    "                num_classes,\n",
    "                vocab_size,\n",
    "                embedding_size,\n",
    "                filter_sizes,\n",
    "                num_filters):\n",
    "        \n",
    "        \n",
    "        # 1) placeholder(x = [None, sequence_length])\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        \n",
    "        # 2) embedding layer(tensor = [None, sequence_length, embedding_size])\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            \n",
    "\n",
    "        # 3) convolution and max-pooling layers\n",
    "        pooled_outputs = []\n",
    "        for i,filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope('conv-maxpool-%s'%filter_size):\n",
    "                # convolution layer(tensor = [None, sequence_length-filter_size+1, 1, num_filter])\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding='VALID'\n",
    "                )\n",
    "                \n",
    "                # apply nonlinearlity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv,b))\n",
    "                \n",
    "                # max-pooling over the outputs(tensor = [None, 1, 1, num_filter])\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1,1,1,1],\n",
    "                    padding='VALID'\n",
    "                )\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "                \n",
    "        # 4) combine all the pooled features(tensor = [None, num_filter*len(filter_sizes)])\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        \n",
    "        # 5) dropout layer(tensor = [None, num_filters_total])\n",
    "        with tf.name_scope('dropout'):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "            \n",
    "        # 6) output layer(fully connected layer, tensor = [None, num_classes])\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1))\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b)\n",
    "            self.predictions = tf.argmax(self.scores, 1)\n",
    "\n",
    "            \n",
    "        # 7) loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits = self.scores, labels = self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "            \n",
    "        # 8) accuracy\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/hist is illegal; using embedding/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/Variable:0/grad/sparsity is illegal; using embedding/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/hist is illegal; using conv-maxpool-3/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/hist is illegal; using conv-maxpool-3/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-3/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/hist is illegal; using conv-maxpool-4/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/hist is illegal; using conv-maxpool-4/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-4/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/hist is illegal; using conv-maxpool-5/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/hist is illegal; using conv-maxpool-5/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/Variable_1:0/grad/sparsity is illegal; using conv-maxpool-5/Variable_1_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/hist is illegal; using output/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/Variable:0/grad/sparsity is illegal; using output/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/Variable_1:0/grad/hist is illegal; using output/Variable_1_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/Variable_1:0/grad/sparsity is illegal; using output/Variable_1_0/grad/sparsity instead.\n",
      "writing to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246\n",
      "\n",
      "2017-04-07T00:57:28.812279: step 1, loss 4.61008, acc 0.45\n",
      "2017-04-07T00:57:29.133231: step 2, loss 2.60137, acc 0.53\n",
      "2017-04-07T00:57:29.416734: step 3, loss 2.52096, acc 0.49\n",
      "2017-04-07T00:57:29.735807: step 4, loss 2.3724, acc 0.49\n",
      "2017-04-07T00:57:30.017194: step 5, loss 2.10086, acc 0.47\n",
      "2017-04-07T00:57:30.334935: step 6, loss 2.9612, acc 0.41\n",
      "2017-04-07T00:57:30.608298: step 7, loss 2.4773, acc 0.55\n",
      "2017-04-07T00:57:30.880116: step 8, loss 2.77394, acc 0.54\n",
      "2017-04-07T00:57:31.157620: step 9, loss 2.28121, acc 0.49\n",
      "2017-04-07T00:57:31.431484: step 10, loss 2.11841, acc 0.59\n",
      "2017-04-07T00:57:31.713214: step 11, loss 1.90069, acc 0.54\n",
      "2017-04-07T00:57:31.998340: step 12, loss 2.25544, acc 0.48\n",
      "2017-04-07T00:57:32.280691: step 13, loss 1.59129, acc 0.56\n",
      "2017-04-07T00:57:32.569042: step 14, loss 2.26777, acc 0.46\n",
      "2017-04-07T00:57:32.853207: step 15, loss 2.11643, acc 0.51\n",
      "2017-04-07T00:57:33.138132: step 16, loss 1.90912, acc 0.51\n",
      "2017-04-07T00:57:33.424214: step 17, loss 1.86578, acc 0.56\n",
      "2017-04-07T00:57:33.703242: step 18, loss 2.01134, acc 0.51\n",
      "2017-04-07T00:57:34.044026: step 19, loss 2.1731, acc 0.44\n",
      "2017-04-07T00:57:34.358030: step 20, loss 1.77242, acc 0.56\n",
      "2017-04-07T00:57:34.631428: step 21, loss 2.06374, acc 0.41\n",
      "2017-04-07T00:57:34.949654: step 22, loss 2.00495, acc 0.46\n",
      "2017-04-07T00:57:35.258446: step 23, loss 2.15441, acc 0.47\n",
      "2017-04-07T00:57:35.590213: step 24, loss 2.20413, acc 0.5\n",
      "2017-04-07T00:57:35.896019: step 25, loss 1.57949, acc 0.53\n",
      "2017-04-07T00:57:36.190230: step 26, loss 1.60986, acc 0.58\n",
      "2017-04-07T00:57:36.478718: step 27, loss 1.81252, acc 0.5\n",
      "2017-04-07T00:57:36.751632: step 28, loss 1.95331, acc 0.55\n",
      "2017-04-07T00:57:37.033316: step 29, loss 1.88735, acc 0.53\n",
      "2017-04-07T00:57:37.314124: step 30, loss 1.58926, acc 0.53\n",
      "2017-04-07T00:57:37.591620: step 31, loss 1.96579, acc 0.43\n",
      "2017-04-07T00:57:37.870244: step 32, loss 1.69882, acc 0.56\n",
      "2017-04-07T00:57:38.151538: step 33, loss 1.59842, acc 0.56\n",
      "2017-04-07T00:57:38.429692: step 34, loss 1.61679, acc 0.5\n",
      "2017-04-07T00:57:38.711023: step 35, loss 1.6934, acc 0.55\n",
      "2017-04-07T00:57:38.995455: step 36, loss 1.68633, acc 0.54\n",
      "2017-04-07T00:57:39.275355: step 37, loss 1.75304, acc 0.54\n",
      "2017-04-07T00:57:39.558151: step 38, loss 1.89496, acc 0.5\n",
      "2017-04-07T00:57:39.836878: step 39, loss 1.65871, acc 0.6\n",
      "2017-04-07T00:57:40.112975: step 40, loss 2.13711, acc 0.47\n",
      "2017-04-07T00:57:40.393945: step 41, loss 1.84801, acc 0.53\n",
      "2017-04-07T00:57:40.666543: step 42, loss 1.95963, acc 0.46\n",
      "2017-04-07T00:57:40.949324: step 43, loss 1.37733, acc 0.6\n",
      "2017-04-07T00:57:41.232071: step 44, loss 1.60282, acc 0.6\n",
      "2017-04-07T00:57:41.517042: step 45, loss 1.39763, acc 0.57\n",
      "2017-04-07T00:57:41.900788: step 46, loss 1.79611, acc 0.48\n",
      "2017-04-07T00:57:42.215666: step 47, loss 1.70127, acc 0.52\n",
      "2017-04-07T00:57:42.490177: step 48, loss 1.96552, acc 0.49\n",
      "2017-04-07T00:57:42.791955: step 49, loss 1.35974, acc 0.59\n",
      "2017-04-07T00:57:43.070142: step 50, loss 2.01966, acc 0.47\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:57:43.852778: step 50, loss 0.797718, acc 0.548781\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-50\n",
      "\n",
      "2017-04-07T00:57:44.693783: step 51, loss 1.76239, acc 0.5\n",
      "2017-04-07T00:57:45.066119: step 52, loss 2.07694, acc 0.47\n",
      "2017-04-07T00:57:45.360662: step 53, loss 1.96521, acc 0.55\n",
      "2017-04-07T00:57:45.703492: step 54, loss 2.01809, acc 0.43\n",
      "2017-04-07T00:57:45.985530: step 55, loss 1.45498, acc 0.54\n",
      "2017-04-07T00:57:46.366065: step 56, loss 1.68419, acc 0.49\n",
      "2017-04-07T00:57:46.654812: step 57, loss 1.37627, acc 0.58\n",
      "2017-04-07T00:57:46.940228: step 58, loss 1.94675, acc 0.52\n",
      "2017-04-07T00:57:47.254518: step 59, loss 1.59203, acc 0.52\n",
      "2017-04-07T00:57:47.539580: step 60, loss 1.56545, acc 0.53\n",
      "2017-04-07T00:57:47.911063: step 61, loss 1.48184, acc 0.57\n",
      "2017-04-07T00:57:48.184758: step 62, loss 1.80541, acc 0.46\n",
      "2017-04-07T00:57:48.468524: step 63, loss 1.5448, acc 0.56\n",
      "2017-04-07T00:57:48.777438: step 64, loss 1.66629, acc 0.53\n",
      "2017-04-07T00:57:49.172657: step 65, loss 1.55412, acc 0.57\n",
      "2017-04-07T00:57:49.458017: step 66, loss 1.75208, acc 0.53\n",
      "2017-04-07T00:57:49.835808: step 67, loss 1.74443, acc 0.49\n",
      "2017-04-07T00:57:50.200972: step 68, loss 1.38215, acc 0.6\n",
      "2017-04-07T00:57:50.480615: step 69, loss 1.34346, acc 0.59\n",
      "2017-04-07T00:57:50.769744: step 70, loss 1.98603, acc 0.49\n",
      "2017-04-07T00:57:51.153339: step 71, loss 1.42436, acc 0.53\n",
      "2017-04-07T00:57:51.437619: step 72, loss 1.54178, acc 0.54\n",
      "2017-04-07T00:57:51.740127: step 73, loss 1.15184, acc 0.55\n",
      "2017-04-07T00:57:52.020704: step 74, loss 1.70623, acc 0.47\n",
      "2017-04-07T00:57:52.387026: step 75, loss 1.61999, acc 0.49\n",
      "2017-04-07T00:57:52.661351: step 76, loss 1.58995, acc 0.56\n",
      "2017-04-07T00:57:52.953123: step 77, loss 1.79735, acc 0.42\n",
      "2017-04-07T00:57:53.321127: step 78, loss 1.52996, acc 0.57\n",
      "2017-04-07T00:57:53.597372: step 79, loss 1.86476, acc 0.49\n",
      "2017-04-07T00:57:53.880997: step 80, loss 1.69547, acc 0.45\n",
      "2017-04-07T00:57:54.262567: step 81, loss 1.8116, acc 0.45\n",
      "2017-04-07T00:57:54.643580: step 82, loss 1.46978, acc 0.51\n",
      "2017-04-07T00:57:54.955792: step 83, loss 1.72411, acc 0.49\n",
      "2017-04-07T00:57:55.257214: step 84, loss 1.36979, acc 0.57\n",
      "2017-04-07T00:57:55.591163: step 85, loss 1.31908, acc 0.57\n",
      "2017-04-07T00:57:55.915303: step 86, loss 1.30378, acc 0.62\n",
      "2017-04-07T00:57:56.201651: step 87, loss 1.7231, acc 0.48\n",
      "2017-04-07T00:57:56.543097: step 88, loss 1.74796, acc 0.46\n",
      "2017-04-07T00:57:56.813198: step 89, loss 1.41369, acc 0.56\n",
      "2017-04-07T00:57:57.123226: step 90, loss 1.39545, acc 0.56\n",
      "2017-04-07T00:57:57.432986: step 91, loss 1.43999, acc 0.54\n",
      "2017-04-07T00:57:57.705114: step 92, loss 1.49287, acc 0.54\n",
      "2017-04-07T00:57:58.043898: step 93, loss 1.46937, acc 0.56\n",
      "2017-04-07T00:57:58.311627: step 94, loss 1.50591, acc 0.53\n",
      "2017-04-07T00:57:58.584706: step 95, loss 1.3712, acc 0.55\n",
      "2017-04-07T00:57:58.867321: step 96, loss 1.6467, acc 0.479167\n",
      "2017-04-07T00:57:59.136284: step 97, loss 1.31242, acc 0.62\n",
      "2017-04-07T00:57:59.409222: step 98, loss 1.31323, acc 0.56\n",
      "2017-04-07T00:57:59.674855: step 99, loss 1.35939, acc 0.56\n",
      "2017-04-07T00:58:00.047562: step 100, loss 1.07977, acc 0.57\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:58:00.914553: step 100, loss 0.736102, acc 0.590056\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-100\n",
      "\n",
      "2017-04-07T00:58:01.786713: step 101, loss 1.3069, acc 0.59\n",
      "2017-04-07T00:58:02.121985: step 102, loss 1.36442, acc 0.59\n",
      "2017-04-07T00:58:02.402212: step 103, loss 1.234, acc 0.56\n",
      "2017-04-07T00:58:02.688368: step 104, loss 1.25036, acc 0.59\n",
      "2017-04-07T00:58:02.981171: step 105, loss 1.52638, acc 0.51\n",
      "2017-04-07T00:58:03.279447: step 106, loss 0.837987, acc 0.68\n",
      "2017-04-07T00:58:03.558196: step 107, loss 1.16443, acc 0.6\n",
      "2017-04-07T00:58:03.840088: step 108, loss 1.26269, acc 0.63\n",
      "2017-04-07T00:58:04.164596: step 109, loss 1.34425, acc 0.53\n",
      "2017-04-07T00:58:04.433372: step 110, loss 1.48216, acc 0.52\n",
      "2017-04-07T00:58:04.788779: step 111, loss 1.02097, acc 0.63\n",
      "2017-04-07T00:58:05.060320: step 112, loss 0.946724, acc 0.65\n",
      "2017-04-07T00:58:05.332867: step 113, loss 1.05583, acc 0.63\n",
      "2017-04-07T00:58:05.600563: step 114, loss 1.17675, acc 0.64\n",
      "2017-04-07T00:58:05.868877: step 115, loss 1.17607, acc 0.61\n",
      "2017-04-07T00:58:06.166610: step 116, loss 0.967891, acc 0.69\n",
      "2017-04-07T00:58:06.471117: step 117, loss 1.09835, acc 0.54\n",
      "2017-04-07T00:58:06.815526: step 118, loss 0.860924, acc 0.67\n",
      "2017-04-07T00:58:07.134631: step 119, loss 1.21904, acc 0.55\n",
      "2017-04-07T00:58:07.458446: step 120, loss 1.43943, acc 0.54\n",
      "2017-04-07T00:58:07.791898: step 121, loss 1.05486, acc 0.61\n",
      "2017-04-07T00:58:08.117142: step 122, loss 1.16693, acc 0.61\n",
      "2017-04-07T00:58:08.425841: step 123, loss 1.04253, acc 0.6\n",
      "2017-04-07T00:58:08.741921: step 124, loss 1.3099, acc 0.51\n",
      "2017-04-07T00:58:09.115532: step 125, loss 1.10387, acc 0.63\n",
      "2017-04-07T00:58:09.410024: step 126, loss 1.06799, acc 0.68\n",
      "2017-04-07T00:58:09.757918: step 127, loss 1.042, acc 0.62\n",
      "2017-04-07T00:58:10.062945: step 128, loss 1.18397, acc 0.62\n",
      "2017-04-07T00:58:10.356643: step 129, loss 1.13082, acc 0.59\n",
      "2017-04-07T00:58:10.677393: step 130, loss 0.906332, acc 0.72\n",
      "2017-04-07T00:58:10.944437: step 131, loss 0.979907, acc 0.64\n",
      "2017-04-07T00:58:11.306709: step 132, loss 1.26729, acc 0.54\n",
      "2017-04-07T00:58:11.624769: step 133, loss 1.09662, acc 0.63\n",
      "2017-04-07T00:58:11.936002: step 134, loss 1.14085, acc 0.54\n",
      "2017-04-07T00:58:12.245685: step 135, loss 1.16178, acc 0.53\n",
      "2017-04-07T00:58:12.552940: step 136, loss 1.06749, acc 0.62\n",
      "2017-04-07T00:58:12.975580: step 137, loss 1.11503, acc 0.61\n",
      "2017-04-07T00:58:13.397412: step 138, loss 0.982205, acc 0.6\n",
      "2017-04-07T00:58:13.757469: step 139, loss 1.11406, acc 0.55\n",
      "2017-04-07T00:58:14.080015: step 140, loss 1.30373, acc 0.56\n",
      "2017-04-07T00:58:14.359210: step 141, loss 1.2206, acc 0.61\n",
      "2017-04-07T00:58:14.664344: step 142, loss 1.2603, acc 0.57\n",
      "2017-04-07T00:58:14.962069: step 143, loss 0.856275, acc 0.59\n",
      "2017-04-07T00:58:15.265179: step 144, loss 1.24565, acc 0.58\n",
      "2017-04-07T00:58:15.544435: step 145, loss 1.14494, acc 0.59\n",
      "2017-04-07T00:58:15.851335: step 146, loss 1.16076, acc 0.64\n",
      "2017-04-07T00:58:16.193484: step 147, loss 1.15447, acc 0.59\n",
      "2017-04-07T00:58:16.640245: step 148, loss 1.05676, acc 0.59\n",
      "2017-04-07T00:58:16.931834: step 149, loss 0.774942, acc 0.67\n",
      "2017-04-07T00:58:17.270621: step 150, loss 1.10394, acc 0.59\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:58:18.225062: step 150, loss 0.706924, acc 0.615385\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-150\n",
      "\n",
      "2017-04-07T00:58:19.123131: step 151, loss 1.12956, acc 0.57\n",
      "2017-04-07T00:58:19.574890: step 152, loss 0.935914, acc 0.65\n",
      "2017-04-07T00:58:19.920792: step 153, loss 0.947987, acc 0.6\n",
      "2017-04-07T00:58:20.240019: step 154, loss 1.08928, acc 0.53\n",
      "2017-04-07T00:58:20.638240: step 155, loss 1.20643, acc 0.61\n",
      "2017-04-07T00:58:21.034301: step 156, loss 1.2573, acc 0.61\n",
      "2017-04-07T00:58:21.329879: step 157, loss 1.28753, acc 0.43\n",
      "2017-04-07T00:58:21.654549: step 158, loss 0.995066, acc 0.59\n",
      "2017-04-07T00:58:22.014174: step 159, loss 1.09657, acc 0.6\n",
      "2017-04-07T00:58:22.357472: step 160, loss 1.10463, acc 0.57\n",
      "2017-04-07T00:58:22.642251: step 161, loss 0.913175, acc 0.59\n",
      "2017-04-07T00:58:22.905180: step 162, loss 0.940139, acc 0.65\n",
      "2017-04-07T00:58:23.174247: step 163, loss 1.37168, acc 0.5\n",
      "2017-04-07T00:58:23.436835: step 164, loss 0.891599, acc 0.64\n",
      "2017-04-07T00:58:23.700913: step 165, loss 0.991064, acc 0.57\n",
      "2017-04-07T00:58:23.962038: step 166, loss 0.860315, acc 0.58\n",
      "2017-04-07T00:58:24.221266: step 167, loss 1.00548, acc 0.63\n",
      "2017-04-07T00:58:24.484262: step 168, loss 0.975776, acc 0.68\n",
      "2017-04-07T00:58:24.746182: step 169, loss 0.95753, acc 0.59\n",
      "2017-04-07T00:58:25.011591: step 170, loss 1.06411, acc 0.54\n",
      "2017-04-07T00:58:25.270570: step 171, loss 0.846962, acc 0.66\n",
      "2017-04-07T00:58:25.534846: step 172, loss 1.21955, acc 0.54\n",
      "2017-04-07T00:58:25.801960: step 173, loss 1.07421, acc 0.53\n",
      "2017-04-07T00:58:26.079075: step 174, loss 1.01059, acc 0.62\n",
      "2017-04-07T00:58:26.388073: step 175, loss 1.22336, acc 0.53\n",
      "2017-04-07T00:58:26.705553: step 176, loss 1.27035, acc 0.56\n",
      "2017-04-07T00:58:27.022579: step 177, loss 1.15402, acc 0.56\n",
      "2017-04-07T00:58:27.334359: step 178, loss 1.16691, acc 0.55\n",
      "2017-04-07T00:58:27.609386: step 179, loss 0.863381, acc 0.61\n",
      "2017-04-07T00:58:27.870708: step 180, loss 1.25248, acc 0.59\n",
      "2017-04-07T00:58:28.131358: step 181, loss 1.36662, acc 0.52\n",
      "2017-04-07T00:58:28.393280: step 182, loss 1.18246, acc 0.55\n",
      "2017-04-07T00:58:28.660374: step 183, loss 0.885791, acc 0.62\n",
      "2017-04-07T00:58:28.936324: step 184, loss 0.653486, acc 0.68\n",
      "2017-04-07T00:58:29.203174: step 185, loss 1.18569, acc 0.55\n",
      "2017-04-07T00:58:29.465669: step 186, loss 1.04441, acc 0.57\n",
      "2017-04-07T00:58:29.727476: step 187, loss 1.16387, acc 0.54\n",
      "2017-04-07T00:58:29.991976: step 188, loss 0.919381, acc 0.64\n",
      "2017-04-07T00:58:30.257646: step 189, loss 0.949325, acc 0.62\n",
      "2017-04-07T00:58:30.519751: step 190, loss 1.02452, acc 0.54\n",
      "2017-04-07T00:58:30.781134: step 191, loss 1.2024, acc 0.52\n",
      "2017-04-07T00:58:31.050857: step 192, loss 1.2047, acc 0.572917\n",
      "2017-04-07T00:58:31.313025: step 193, loss 1.04546, acc 0.57\n",
      "2017-04-07T00:58:31.574268: step 194, loss 1.14696, acc 0.54\n",
      "2017-04-07T00:58:31.843178: step 195, loss 1.06802, acc 0.6\n",
      "2017-04-07T00:58:32.110431: step 196, loss 0.866093, acc 0.64\n",
      "2017-04-07T00:58:32.380408: step 197, loss 0.803377, acc 0.65\n",
      "2017-04-07T00:58:32.651910: step 198, loss 0.79641, acc 0.66\n",
      "2017-04-07T00:58:32.918389: step 199, loss 1.03088, acc 0.58\n",
      "2017-04-07T00:58:33.183886: step 200, loss 0.731841, acc 0.68\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:58:33.829193: step 200, loss 0.680978, acc 0.603189\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-200\n",
      "\n",
      "2017-04-07T00:58:34.606275: step 201, loss 0.810733, acc 0.63\n",
      "2017-04-07T00:58:34.870933: step 202, loss 0.827484, acc 0.66\n",
      "2017-04-07T00:58:35.137321: step 203, loss 0.919584, acc 0.6\n",
      "2017-04-07T00:58:35.402453: step 204, loss 0.921045, acc 0.62\n",
      "2017-04-07T00:58:35.665935: step 205, loss 0.721005, acc 0.67\n",
      "2017-04-07T00:58:35.937278: step 206, loss 1.03201, acc 0.61\n",
      "2017-04-07T00:58:36.205019: step 207, loss 1.10948, acc 0.53\n",
      "2017-04-07T00:58:36.466339: step 208, loss 0.917428, acc 0.59\n",
      "2017-04-07T00:58:36.730553: step 209, loss 0.883092, acc 0.58\n",
      "2017-04-07T00:58:37.010766: step 210, loss 0.711629, acc 0.7\n",
      "2017-04-07T00:58:37.275066: step 211, loss 0.948875, acc 0.61\n",
      "2017-04-07T00:58:37.567359: step 212, loss 0.936044, acc 0.63\n",
      "2017-04-07T00:58:37.830573: step 213, loss 0.99526, acc 0.5\n",
      "2017-04-07T00:58:38.097238: step 214, loss 0.972772, acc 0.55\n",
      "2017-04-07T00:58:38.367361: step 215, loss 0.764004, acc 0.64\n",
      "2017-04-07T00:58:38.633808: step 216, loss 0.624536, acc 0.69\n",
      "2017-04-07T00:58:38.900261: step 217, loss 0.822148, acc 0.63\n",
      "2017-04-07T00:58:39.159860: step 218, loss 0.849808, acc 0.59\n",
      "2017-04-07T00:58:39.419986: step 219, loss 0.791016, acc 0.67\n",
      "2017-04-07T00:58:39.686530: step 220, loss 0.885256, acc 0.63\n",
      "2017-04-07T00:58:39.952013: step 221, loss 0.632937, acc 0.69\n",
      "2017-04-07T00:58:40.218748: step 222, loss 0.800544, acc 0.61\n",
      "2017-04-07T00:58:40.485383: step 223, loss 0.857759, acc 0.65\n",
      "2017-04-07T00:58:40.749692: step 224, loss 0.832411, acc 0.62\n",
      "2017-04-07T00:58:41.017700: step 225, loss 0.931741, acc 0.56\n",
      "2017-04-07T00:58:41.277779: step 226, loss 0.952656, acc 0.61\n",
      "2017-04-07T00:58:41.541583: step 227, loss 0.784464, acc 0.66\n",
      "2017-04-07T00:58:41.812405: step 228, loss 0.957268, acc 0.57\n",
      "2017-04-07T00:58:42.080177: step 229, loss 0.703183, acc 0.68\n",
      "2017-04-07T00:58:42.347950: step 230, loss 0.591224, acc 0.77\n",
      "2017-04-07T00:58:42.613971: step 231, loss 0.799206, acc 0.65\n",
      "2017-04-07T00:58:42.877959: step 232, loss 0.83908, acc 0.54\n",
      "2017-04-07T00:58:43.144360: step 233, loss 0.654528, acc 0.65\n",
      "2017-04-07T00:58:43.415252: step 234, loss 0.925031, acc 0.59\n",
      "2017-04-07T00:58:43.680641: step 235, loss 0.755192, acc 0.58\n",
      "2017-04-07T00:58:43.943694: step 236, loss 0.890802, acc 0.65\n",
      "2017-04-07T00:58:44.204384: step 237, loss 0.998443, acc 0.57\n",
      "2017-04-07T00:58:44.464309: step 238, loss 0.768592, acc 0.65\n",
      "2017-04-07T00:58:44.726755: step 239, loss 0.928199, acc 0.65\n",
      "2017-04-07T00:58:44.993678: step 240, loss 0.788497, acc 0.59\n",
      "2017-04-07T00:58:45.259897: step 241, loss 0.705709, acc 0.68\n",
      "2017-04-07T00:58:45.521465: step 242, loss 0.803672, acc 0.64\n",
      "2017-04-07T00:58:45.786070: step 243, loss 0.636902, acc 0.71\n",
      "2017-04-07T00:58:46.060741: step 244, loss 0.917371, acc 0.61\n",
      "2017-04-07T00:58:46.321053: step 245, loss 0.780677, acc 0.67\n",
      "2017-04-07T00:58:46.581113: step 246, loss 0.763025, acc 0.62\n",
      "2017-04-07T00:58:46.846505: step 247, loss 1.00162, acc 0.58\n",
      "2017-04-07T00:58:47.115032: step 248, loss 0.810128, acc 0.67\n",
      "2017-04-07T00:58:47.378691: step 249, loss 0.633852, acc 0.71\n",
      "2017-04-07T00:58:47.643076: step 250, loss 0.878393, acc 0.63\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:58:48.304014: step 250, loss 0.646416, acc 0.619137\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-250\n",
      "\n",
      "2017-04-07T00:58:49.087953: step 251, loss 0.826521, acc 0.65\n",
      "2017-04-07T00:58:49.356115: step 252, loss 0.888284, acc 0.65\n",
      "2017-04-07T00:58:49.619044: step 253, loss 0.801821, acc 0.59\n",
      "2017-04-07T00:58:49.886326: step 254, loss 0.916637, acc 0.49\n",
      "2017-04-07T00:58:50.153415: step 255, loss 0.844338, acc 0.65\n",
      "2017-04-07T00:58:50.415518: step 256, loss 0.648706, acc 0.7\n",
      "2017-04-07T00:58:50.678196: step 257, loss 0.815127, acc 0.58\n",
      "2017-04-07T00:58:50.945585: step 258, loss 0.546418, acc 0.7\n",
      "2017-04-07T00:58:51.205937: step 259, loss 0.752419, acc 0.68\n",
      "2017-04-07T00:58:51.471513: step 260, loss 0.640946, acc 0.64\n",
      "2017-04-07T00:58:51.737670: step 261, loss 0.795669, acc 0.65\n",
      "2017-04-07T00:58:52.014953: step 262, loss 0.934321, acc 0.56\n",
      "2017-04-07T00:58:52.282127: step 263, loss 0.955716, acc 0.62\n",
      "2017-04-07T00:58:52.560815: step 264, loss 0.694437, acc 0.6\n",
      "2017-04-07T00:58:52.835156: step 265, loss 0.849766, acc 0.61\n",
      "2017-04-07T00:58:53.096280: step 266, loss 0.810601, acc 0.64\n",
      "2017-04-07T00:58:53.359067: step 267, loss 0.842894, acc 0.59\n",
      "2017-04-07T00:58:53.625459: step 268, loss 0.713112, acc 0.64\n",
      "2017-04-07T00:58:53.889204: step 269, loss 0.941054, acc 0.57\n",
      "2017-04-07T00:58:54.148901: step 270, loss 0.656836, acc 0.68\n",
      "2017-04-07T00:58:54.412346: step 271, loss 0.820893, acc 0.66\n",
      "2017-04-07T00:58:54.676490: step 272, loss 0.7508, acc 0.69\n",
      "2017-04-07T00:58:54.941319: step 273, loss 0.618009, acc 0.7\n",
      "2017-04-07T00:58:55.203626: step 274, loss 0.894822, acc 0.52\n",
      "2017-04-07T00:58:55.465616: step 275, loss 0.870222, acc 0.64\n",
      "2017-04-07T00:58:55.727389: step 276, loss 0.642744, acc 0.7\n",
      "2017-04-07T00:58:55.998322: step 277, loss 0.764741, acc 0.67\n",
      "2017-04-07T00:58:56.259270: step 278, loss 0.706666, acc 0.63\n",
      "2017-04-07T00:58:56.524526: step 279, loss 0.655103, acc 0.69\n",
      "2017-04-07T00:58:56.789390: step 280, loss 0.632157, acc 0.67\n",
      "2017-04-07T00:58:57.054744: step 281, loss 0.634983, acc 0.71\n",
      "2017-04-07T00:58:57.322123: step 282, loss 0.681027, acc 0.69\n",
      "2017-04-07T00:58:57.584862: step 283, loss 0.639221, acc 0.71\n",
      "2017-04-07T00:58:57.845588: step 284, loss 0.733955, acc 0.71\n",
      "2017-04-07T00:58:58.112665: step 285, loss 0.780178, acc 0.66\n",
      "2017-04-07T00:58:58.374900: step 286, loss 0.775344, acc 0.64\n",
      "2017-04-07T00:58:58.641607: step 287, loss 0.798752, acc 0.65\n",
      "2017-04-07T00:58:58.900547: step 288, loss 0.725196, acc 0.635417\n",
      "2017-04-07T00:58:59.170943: step 289, loss 0.745593, acc 0.64\n",
      "2017-04-07T00:58:59.430372: step 290, loss 0.753492, acc 0.62\n",
      "2017-04-07T00:58:59.694125: step 291, loss 0.771216, acc 0.63\n",
      "2017-04-07T00:58:59.964340: step 292, loss 0.574314, acc 0.71\n",
      "2017-04-07T00:59:00.231745: step 293, loss 0.67632, acc 0.7\n",
      "2017-04-07T00:59:00.500370: step 294, loss 0.682367, acc 0.61\n",
      "2017-04-07T00:59:00.766274: step 295, loss 0.756861, acc 0.56\n",
      "2017-04-07T00:59:01.045376: step 296, loss 0.523065, acc 0.72\n",
      "2017-04-07T00:59:01.309421: step 297, loss 0.608658, acc 0.69\n",
      "2017-04-07T00:59:01.568563: step 298, loss 0.635935, acc 0.69\n",
      "2017-04-07T00:59:01.832457: step 299, loss 0.748268, acc 0.6\n",
      "2017-04-07T00:59:02.097098: step 300, loss 0.723194, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:59:02.752567: step 300, loss 0.639163, acc 0.636023\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-300\n",
      "\n",
      "2017-04-07T00:59:03.541214: step 301, loss 0.662453, acc 0.65\n",
      "2017-04-07T00:59:03.806291: step 302, loss 0.770982, acc 0.66\n",
      "2017-04-07T00:59:04.073332: step 303, loss 0.680737, acc 0.66\n",
      "2017-04-07T00:59:04.339292: step 304, loss 0.732541, acc 0.63\n",
      "2017-04-07T00:59:04.602370: step 305, loss 0.503486, acc 0.77\n",
      "2017-04-07T00:59:04.864943: step 306, loss 0.681139, acc 0.58\n",
      "2017-04-07T00:59:05.134312: step 307, loss 0.680718, acc 0.7\n",
      "2017-04-07T00:59:05.402589: step 308, loss 0.68676, acc 0.67\n",
      "2017-04-07T00:59:05.669891: step 309, loss 0.872966, acc 0.52\n",
      "2017-04-07T00:59:05.938606: step 310, loss 0.592453, acc 0.73\n",
      "2017-04-07T00:59:06.210834: step 311, loss 0.553934, acc 0.7\n",
      "2017-04-07T00:59:06.479155: step 312, loss 0.639629, acc 0.73\n",
      "2017-04-07T00:59:06.743059: step 313, loss 0.579057, acc 0.71\n",
      "2017-04-07T00:59:07.005493: step 314, loss 0.668263, acc 0.69\n",
      "2017-04-07T00:59:07.268079: step 315, loss 0.695684, acc 0.61\n",
      "2017-04-07T00:59:07.540613: step 316, loss 0.649843, acc 0.63\n",
      "2017-04-07T00:59:07.802559: step 317, loss 0.640204, acc 0.73\n",
      "2017-04-07T00:59:08.066505: step 318, loss 0.694068, acc 0.65\n",
      "2017-04-07T00:59:08.329617: step 319, loss 0.587687, acc 0.72\n",
      "2017-04-07T00:59:08.589720: step 320, loss 0.568427, acc 0.7\n",
      "2017-04-07T00:59:08.853021: step 321, loss 0.596072, acc 0.71\n",
      "2017-04-07T00:59:09.121192: step 322, loss 0.616998, acc 0.72\n",
      "2017-04-07T00:59:09.382666: step 323, loss 0.654896, acc 0.69\n",
      "2017-04-07T00:59:09.644089: step 324, loss 0.645862, acc 0.68\n",
      "2017-04-07T00:59:09.909014: step 325, loss 0.632181, acc 0.68\n",
      "2017-04-07T00:59:10.170132: step 326, loss 0.640485, acc 0.67\n",
      "2017-04-07T00:59:10.432028: step 327, loss 0.597804, acc 0.66\n",
      "2017-04-07T00:59:10.695607: step 328, loss 0.584467, acc 0.72\n",
      "2017-04-07T00:59:10.968208: step 329, loss 0.662552, acc 0.64\n",
      "2017-04-07T00:59:11.230144: step 330, loss 0.752398, acc 0.66\n",
      "2017-04-07T00:59:11.493647: step 331, loss 0.638652, acc 0.69\n",
      "2017-04-07T00:59:11.755396: step 332, loss 0.743074, acc 0.58\n",
      "2017-04-07T00:59:12.022199: step 333, loss 0.581241, acc 0.72\n",
      "2017-04-07T00:59:12.282658: step 334, loss 0.67505, acc 0.66\n",
      "2017-04-07T00:59:12.542991: step 335, loss 0.734034, acc 0.62\n",
      "2017-04-07T00:59:12.806189: step 336, loss 0.600579, acc 0.69\n",
      "2017-04-07T00:59:13.070034: step 337, loss 0.705712, acc 0.63\n",
      "2017-04-07T00:59:13.336632: step 338, loss 0.553417, acc 0.75\n",
      "2017-04-07T00:59:13.599505: step 339, loss 0.806743, acc 0.62\n",
      "2017-04-07T00:59:13.862499: step 340, loss 0.597264, acc 0.7\n",
      "2017-04-07T00:59:14.128164: step 341, loss 0.65198, acc 0.69\n",
      "2017-04-07T00:59:14.391436: step 342, loss 0.901015, acc 0.58\n",
      "2017-04-07T00:59:14.653529: step 343, loss 0.594729, acc 0.72\n",
      "2017-04-07T00:59:14.914445: step 344, loss 0.733266, acc 0.64\n",
      "2017-04-07T00:59:15.177612: step 345, loss 0.593476, acc 0.74\n",
      "2017-04-07T00:59:15.440224: step 346, loss 0.530644, acc 0.77\n",
      "2017-04-07T00:59:15.704980: step 347, loss 0.652803, acc 0.73\n",
      "2017-04-07T00:59:15.983078: step 348, loss 0.562547, acc 0.74\n",
      "2017-04-07T00:59:16.249543: step 349, loss 0.607296, acc 0.68\n",
      "2017-04-07T00:59:16.511272: step 350, loss 0.551593, acc 0.73\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:59:17.168786: step 350, loss 0.644103, acc 0.63227\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-350\n",
      "\n",
      "2017-04-07T00:59:17.968256: step 351, loss 0.697864, acc 0.61\n",
      "2017-04-07T00:59:18.241908: step 352, loss 0.666938, acc 0.65\n",
      "2017-04-07T00:59:18.504573: step 353, loss 0.726054, acc 0.63\n",
      "2017-04-07T00:59:18.776935: step 354, loss 0.595389, acc 0.7\n",
      "2017-04-07T00:59:19.054982: step 355, loss 0.630454, acc 0.69\n",
      "2017-04-07T00:59:19.320689: step 356, loss 0.556729, acc 0.7\n",
      "2017-04-07T00:59:19.588537: step 357, loss 0.614808, acc 0.67\n",
      "2017-04-07T00:59:19.854276: step 358, loss 0.590603, acc 0.73\n",
      "2017-04-07T00:59:20.118970: step 359, loss 0.673868, acc 0.66\n",
      "2017-04-07T00:59:20.385875: step 360, loss 0.533484, acc 0.74\n",
      "2017-04-07T00:59:20.648750: step 361, loss 0.539503, acc 0.71\n",
      "2017-04-07T00:59:20.921595: step 362, loss 0.650107, acc 0.66\n",
      "2017-04-07T00:59:21.185258: step 363, loss 0.63103, acc 0.68\n",
      "2017-04-07T00:59:21.451394: step 364, loss 0.5713, acc 0.7\n",
      "2017-04-07T00:59:21.716441: step 365, loss 0.663004, acc 0.71\n",
      "2017-04-07T00:59:21.985178: step 366, loss 0.504232, acc 0.76\n",
      "2017-04-07T00:59:22.253842: step 367, loss 0.545914, acc 0.73\n",
      "2017-04-07T00:59:22.518042: step 368, loss 0.664248, acc 0.68\n",
      "2017-04-07T00:59:22.783360: step 369, loss 0.670617, acc 0.7\n",
      "2017-04-07T00:59:23.059711: step 370, loss 0.657445, acc 0.67\n",
      "2017-04-07T00:59:23.323896: step 371, loss 0.819386, acc 0.6\n",
      "2017-04-07T00:59:23.594444: step 372, loss 0.65589, acc 0.68\n",
      "2017-04-07T00:59:23.859142: step 373, loss 0.550713, acc 0.75\n",
      "2017-04-07T00:59:24.125517: step 374, loss 0.687982, acc 0.63\n",
      "2017-04-07T00:59:24.393055: step 375, loss 0.612438, acc 0.68\n",
      "2017-04-07T00:59:24.657756: step 376, loss 0.62742, acc 0.6\n",
      "2017-04-07T00:59:24.927655: step 377, loss 0.554563, acc 0.76\n",
      "2017-04-07T00:59:25.191387: step 378, loss 0.725926, acc 0.66\n",
      "2017-04-07T00:59:25.454148: step 379, loss 0.649132, acc 0.74\n",
      "2017-04-07T00:59:25.717730: step 380, loss 0.626122, acc 0.67\n",
      "2017-04-07T00:59:25.992672: step 381, loss 0.616017, acc 0.68\n",
      "2017-04-07T00:59:26.259552: step 382, loss 0.695207, acc 0.64\n",
      "2017-04-07T00:59:26.528927: step 383, loss 0.667651, acc 0.63\n",
      "2017-04-07T00:59:26.789623: step 384, loss 0.578999, acc 0.677083\n",
      "2017-04-07T00:59:27.061795: step 385, loss 0.608322, acc 0.68\n",
      "2017-04-07T00:59:27.328493: step 386, loss 0.537681, acc 0.68\n",
      "2017-04-07T00:59:27.596256: step 387, loss 0.594162, acc 0.69\n",
      "2017-04-07T00:59:27.864949: step 388, loss 0.57471, acc 0.72\n",
      "2017-04-07T00:59:28.134116: step 389, loss 0.561067, acc 0.77\n",
      "2017-04-07T00:59:28.402845: step 390, loss 0.558883, acc 0.69\n",
      "2017-04-07T00:59:28.671957: step 391, loss 0.651497, acc 0.68\n",
      "2017-04-07T00:59:28.944627: step 392, loss 0.527692, acc 0.74\n",
      "2017-04-07T00:59:29.266132: step 393, loss 0.499438, acc 0.78\n",
      "2017-04-07T00:59:29.541869: step 394, loss 0.453486, acc 0.8\n",
      "2017-04-07T00:59:29.840979: step 395, loss 0.540966, acc 0.73\n",
      "2017-04-07T00:59:30.177790: step 396, loss 0.538536, acc 0.74\n",
      "2017-04-07T00:59:30.497496: step 397, loss 0.521071, acc 0.76\n",
      "2017-04-07T00:59:30.853058: step 398, loss 0.615282, acc 0.69\n",
      "2017-04-07T00:59:31.143155: step 399, loss 0.541056, acc 0.73\n",
      "2017-04-07T00:59:31.457800: step 400, loss 0.462303, acc 0.82\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:59:32.332607: step 400, loss 0.6163, acc 0.657598\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-400\n",
      "\n",
      "2017-04-07T00:59:33.269331: step 401, loss 0.514962, acc 0.77\n",
      "2017-04-07T00:59:33.552619: step 402, loss 0.582676, acc 0.71\n",
      "2017-04-07T00:59:33.866855: step 403, loss 0.587596, acc 0.66\n",
      "2017-04-07T00:59:34.171368: step 404, loss 0.563237, acc 0.76\n",
      "2017-04-07T00:59:34.452661: step 405, loss 0.530644, acc 0.71\n",
      "2017-04-07T00:59:34.829501: step 406, loss 0.538132, acc 0.74\n",
      "2017-04-07T00:59:35.191900: step 407, loss 0.501753, acc 0.77\n",
      "2017-04-07T00:59:35.553956: step 408, loss 0.580246, acc 0.69\n",
      "2017-04-07T00:59:35.838206: step 409, loss 0.572779, acc 0.71\n",
      "2017-04-07T00:59:36.191329: step 410, loss 0.531495, acc 0.71\n",
      "2017-04-07T00:59:36.466304: step 411, loss 0.560374, acc 0.71\n",
      "2017-04-07T00:59:36.760129: step 412, loss 0.620159, acc 0.66\n",
      "2017-04-07T00:59:37.059571: step 413, loss 0.570205, acc 0.69\n",
      "2017-04-07T00:59:37.322405: step 414, loss 0.530228, acc 0.75\n",
      "2017-04-07T00:59:37.650822: step 415, loss 0.580465, acc 0.68\n",
      "2017-04-07T00:59:37.979093: step 416, loss 0.526898, acc 0.71\n",
      "2017-04-07T00:59:38.263594: step 417, loss 0.559062, acc 0.74\n",
      "2017-04-07T00:59:38.535091: step 418, loss 0.625134, acc 0.64\n",
      "2017-04-07T00:59:38.807286: step 419, loss 0.453498, acc 0.76\n",
      "2017-04-07T00:59:39.155432: step 420, loss 0.485751, acc 0.78\n",
      "2017-04-07T00:59:39.575707: step 421, loss 0.622401, acc 0.72\n",
      "2017-04-07T00:59:39.922900: step 422, loss 0.581901, acc 0.75\n",
      "2017-04-07T00:59:40.202174: step 423, loss 0.557724, acc 0.71\n",
      "2017-04-07T00:59:40.480811: step 424, loss 0.396315, acc 0.85\n",
      "2017-04-07T00:59:40.799847: step 425, loss 0.448831, acc 0.78\n",
      "2017-04-07T00:59:41.099514: step 426, loss 0.627666, acc 0.7\n",
      "2017-04-07T00:59:41.375065: step 427, loss 0.558803, acc 0.7\n",
      "2017-04-07T00:59:41.748936: step 428, loss 0.538264, acc 0.74\n",
      "2017-04-07T00:59:42.069981: step 429, loss 0.619003, acc 0.69\n",
      "2017-04-07T00:59:42.349175: step 430, loss 0.526956, acc 0.7\n",
      "2017-04-07T00:59:42.683127: step 431, loss 0.51451, acc 0.77\n",
      "2017-04-07T00:59:42.978895: step 432, loss 0.499937, acc 0.74\n",
      "2017-04-07T00:59:43.256050: step 433, loss 0.576586, acc 0.68\n",
      "2017-04-07T00:59:43.547015: step 434, loss 0.531063, acc 0.76\n",
      "2017-04-07T00:59:43.821771: step 435, loss 0.548705, acc 0.74\n",
      "2017-04-07T00:59:44.099552: step 436, loss 0.440703, acc 0.83\n",
      "2017-04-07T00:59:44.377914: step 437, loss 0.557557, acc 0.72\n",
      "2017-04-07T00:59:44.659160: step 438, loss 0.619875, acc 0.69\n",
      "2017-04-07T00:59:44.968093: step 439, loss 0.576199, acc 0.67\n",
      "2017-04-07T00:59:45.254785: step 440, loss 0.562203, acc 0.77\n",
      "2017-04-07T00:59:45.522644: step 441, loss 0.625452, acc 0.7\n",
      "2017-04-07T00:59:45.790624: step 442, loss 0.722821, acc 0.64\n",
      "2017-04-07T00:59:46.061578: step 443, loss 0.526354, acc 0.71\n",
      "2017-04-07T00:59:46.330754: step 444, loss 0.595609, acc 0.7\n",
      "2017-04-07T00:59:46.605463: step 445, loss 0.623752, acc 0.66\n",
      "2017-04-07T00:59:46.867396: step 446, loss 0.483717, acc 0.76\n",
      "2017-04-07T00:59:47.143842: step 447, loss 0.529753, acc 0.73\n",
      "2017-04-07T00:59:47.415037: step 448, loss 0.494979, acc 0.74\n",
      "2017-04-07T00:59:47.733105: step 449, loss 0.520893, acc 0.73\n",
      "2017-04-07T00:59:48.049546: step 450, loss 0.565368, acc 0.69\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T00:59:48.879027: step 450, loss 0.616498, acc 0.657598\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-450\n",
      "\n",
      "2017-04-07T00:59:49.775397: step 451, loss 0.542394, acc 0.71\n",
      "2017-04-07T00:59:50.079021: step 452, loss 0.575015, acc 0.69\n",
      "2017-04-07T00:59:50.355984: step 453, loss 0.546937, acc 0.71\n",
      "2017-04-07T00:59:50.629479: step 454, loss 0.632941, acc 0.7\n",
      "2017-04-07T00:59:50.896077: step 455, loss 0.452457, acc 0.77\n",
      "2017-04-07T00:59:51.169696: step 456, loss 0.50616, acc 0.75\n",
      "2017-04-07T00:59:51.436029: step 457, loss 0.682274, acc 0.66\n",
      "2017-04-07T00:59:51.705372: step 458, loss 0.647764, acc 0.62\n",
      "2017-04-07T00:59:51.975966: step 459, loss 0.602209, acc 0.7\n",
      "2017-04-07T00:59:52.243737: step 460, loss 0.533698, acc 0.72\n",
      "2017-04-07T00:59:52.516258: step 461, loss 0.600131, acc 0.64\n",
      "2017-04-07T00:59:52.781283: step 462, loss 0.472718, acc 0.76\n",
      "2017-04-07T00:59:53.054563: step 463, loss 0.59911, acc 0.7\n",
      "2017-04-07T00:59:53.320403: step 464, loss 0.592112, acc 0.69\n",
      "2017-04-07T00:59:53.590929: step 465, loss 0.577041, acc 0.73\n",
      "2017-04-07T00:59:53.864701: step 466, loss 0.722512, acc 0.64\n",
      "2017-04-07T00:59:54.135418: step 467, loss 0.574827, acc 0.72\n",
      "2017-04-07T00:59:54.403860: step 468, loss 0.563971, acc 0.71\n",
      "2017-04-07T00:59:54.673491: step 469, loss 0.513273, acc 0.75\n",
      "2017-04-07T00:59:54.949784: step 470, loss 0.619351, acc 0.69\n",
      "2017-04-07T00:59:55.221816: step 471, loss 0.687302, acc 0.61\n",
      "2017-04-07T00:59:55.489648: step 472, loss 0.67108, acc 0.64\n",
      "2017-04-07T00:59:55.756470: step 473, loss 0.571267, acc 0.69\n",
      "2017-04-07T00:59:56.028398: step 474, loss 0.660638, acc 0.7\n",
      "2017-04-07T00:59:56.294016: step 475, loss 0.60445, acc 0.71\n",
      "2017-04-07T00:59:56.563696: step 476, loss 0.634682, acc 0.65\n",
      "2017-04-07T00:59:56.830017: step 477, loss 0.477921, acc 0.78\n",
      "2017-04-07T00:59:57.103555: step 478, loss 0.580545, acc 0.72\n",
      "2017-04-07T00:59:57.365586: step 479, loss 0.447018, acc 0.75\n",
      "2017-04-07T00:59:57.627852: step 480, loss 0.57046, acc 0.729167\n",
      "2017-04-07T00:59:57.904177: step 481, loss 0.534145, acc 0.79\n",
      "2017-04-07T00:59:58.175793: step 482, loss 0.521085, acc 0.74\n",
      "2017-04-07T00:59:58.447278: step 483, loss 0.578768, acc 0.64\n",
      "2017-04-07T00:59:58.719380: step 484, loss 0.537438, acc 0.69\n",
      "2017-04-07T00:59:58.985439: step 485, loss 0.427487, acc 0.79\n",
      "2017-04-07T00:59:59.260142: step 486, loss 0.527001, acc 0.74\n",
      "2017-04-07T00:59:59.527950: step 487, loss 0.475786, acc 0.75\n",
      "2017-04-07T00:59:59.797376: step 488, loss 0.472137, acc 0.81\n",
      "2017-04-07T01:00:00.070021: step 489, loss 0.485717, acc 0.77\n",
      "2017-04-07T01:00:00.349339: step 490, loss 0.569815, acc 0.72\n",
      "2017-04-07T01:00:00.617182: step 491, loss 0.449711, acc 0.77\n",
      "2017-04-07T01:00:00.888331: step 492, loss 0.474187, acc 0.77\n",
      "2017-04-07T01:00:01.160451: step 493, loss 0.471964, acc 0.74\n",
      "2017-04-07T01:00:01.433752: step 494, loss 0.553239, acc 0.74\n",
      "2017-04-07T01:00:01.701724: step 495, loss 0.57882, acc 0.74\n",
      "2017-04-07T01:00:01.976488: step 496, loss 0.539541, acc 0.7\n",
      "2017-04-07T01:00:02.245526: step 497, loss 0.533668, acc 0.76\n",
      "2017-04-07T01:00:02.520772: step 498, loss 0.503715, acc 0.78\n",
      "2017-04-07T01:00:02.785750: step 499, loss 0.506144, acc 0.74\n",
      "2017-04-07T01:00:03.057373: step 500, loss 0.485372, acc 0.73\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:00:03.771905: step 500, loss 0.603751, acc 0.667917\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-500\n",
      "\n",
      "2017-04-07T01:00:04.594891: step 501, loss 0.540887, acc 0.74\n",
      "2017-04-07T01:00:04.873466: step 502, loss 0.587462, acc 0.71\n",
      "2017-04-07T01:00:05.148239: step 503, loss 0.482793, acc 0.78\n",
      "2017-04-07T01:00:05.417457: step 504, loss 0.437396, acc 0.8\n",
      "2017-04-07T01:00:05.692486: step 505, loss 0.534698, acc 0.75\n",
      "2017-04-07T01:00:05.964898: step 506, loss 0.504738, acc 0.74\n",
      "2017-04-07T01:00:06.237141: step 507, loss 0.494641, acc 0.71\n",
      "2017-04-07T01:00:06.506281: step 508, loss 0.486935, acc 0.75\n",
      "2017-04-07T01:00:06.773984: step 509, loss 0.475335, acc 0.84\n",
      "2017-04-07T01:00:07.044408: step 510, loss 0.460201, acc 0.77\n",
      "2017-04-07T01:00:07.317726: step 511, loss 0.535252, acc 0.71\n",
      "2017-04-07T01:00:07.584612: step 512, loss 0.46586, acc 0.78\n",
      "2017-04-07T01:00:07.862039: step 513, loss 0.535224, acc 0.74\n",
      "2017-04-07T01:00:08.133219: step 514, loss 0.480853, acc 0.73\n",
      "2017-04-07T01:00:08.406552: step 515, loss 0.522571, acc 0.78\n",
      "2017-04-07T01:00:08.676273: step 516, loss 0.560198, acc 0.77\n",
      "2017-04-07T01:00:08.949373: step 517, loss 0.512435, acc 0.75\n",
      "2017-04-07T01:00:09.224381: step 518, loss 0.647413, acc 0.65\n",
      "2017-04-07T01:00:09.494351: step 519, loss 0.489615, acc 0.78\n",
      "2017-04-07T01:00:09.765332: step 520, loss 0.496161, acc 0.74\n",
      "2017-04-07T01:00:10.039961: step 521, loss 0.489176, acc 0.75\n",
      "2017-04-07T01:00:10.312418: step 522, loss 0.461157, acc 0.78\n",
      "2017-04-07T01:00:10.583797: step 523, loss 0.523267, acc 0.71\n",
      "2017-04-07T01:00:10.852500: step 524, loss 0.60308, acc 0.63\n",
      "2017-04-07T01:00:11.129268: step 525, loss 0.375807, acc 0.83\n",
      "2017-04-07T01:00:11.397609: step 526, loss 0.460163, acc 0.72\n",
      "2017-04-07T01:00:11.670566: step 527, loss 0.503067, acc 0.75\n",
      "2017-04-07T01:00:11.948632: step 528, loss 0.517355, acc 0.78\n",
      "2017-04-07T01:00:12.222769: step 529, loss 0.506614, acc 0.78\n",
      "2017-04-07T01:00:12.493304: step 530, loss 0.570825, acc 0.75\n",
      "2017-04-07T01:00:12.760110: step 531, loss 0.51781, acc 0.72\n",
      "2017-04-07T01:00:13.035183: step 532, loss 0.541792, acc 0.77\n",
      "2017-04-07T01:00:13.306913: step 533, loss 0.510699, acc 0.76\n",
      "2017-04-07T01:00:13.576330: step 534, loss 0.588004, acc 0.68\n",
      "2017-04-07T01:00:13.849579: step 535, loss 0.539927, acc 0.7\n",
      "2017-04-07T01:00:14.157149: step 536, loss 0.57956, acc 0.74\n",
      "2017-04-07T01:00:14.539389: step 537, loss 0.448053, acc 0.78\n",
      "2017-04-07T01:00:14.865253: step 538, loss 0.529967, acc 0.72\n",
      "2017-04-07T01:00:15.151750: step 539, loss 0.598269, acc 0.68\n",
      "2017-04-07T01:00:15.427858: step 540, loss 0.507081, acc 0.79\n",
      "2017-04-07T01:00:15.721991: step 541, loss 0.380834, acc 0.84\n",
      "2017-04-07T01:00:15.991452: step 542, loss 0.528598, acc 0.71\n",
      "2017-04-07T01:00:16.271901: step 543, loss 0.458522, acc 0.75\n",
      "2017-04-07T01:00:16.543954: step 544, loss 0.625455, acc 0.68\n",
      "2017-04-07T01:00:16.810653: step 545, loss 0.539795, acc 0.78\n",
      "2017-04-07T01:00:17.084437: step 546, loss 0.530998, acc 0.71\n",
      "2017-04-07T01:00:17.355141: step 547, loss 0.652382, acc 0.68\n",
      "2017-04-07T01:00:17.630760: step 548, loss 0.587917, acc 0.75\n",
      "2017-04-07T01:00:17.902113: step 549, loss 0.50484, acc 0.75\n",
      "2017-04-07T01:00:18.177410: step 550, loss 0.578463, acc 0.71\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:00:18.863990: step 550, loss 0.62439, acc 0.653846\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-550\n",
      "\n",
      "2017-04-07T01:00:19.665826: step 551, loss 0.457099, acc 0.79\n",
      "2017-04-07T01:00:19.941870: step 552, loss 0.531802, acc 0.75\n",
      "2017-04-07T01:00:20.219073: step 553, loss 0.595551, acc 0.65\n",
      "2017-04-07T01:00:20.491077: step 554, loss 0.521542, acc 0.73\n",
      "2017-04-07T01:00:20.769084: step 555, loss 0.569677, acc 0.73\n",
      "2017-04-07T01:00:21.039199: step 556, loss 0.518779, acc 0.76\n",
      "2017-04-07T01:00:21.312611: step 557, loss 0.502978, acc 0.73\n",
      "2017-04-07T01:00:21.585562: step 558, loss 0.540314, acc 0.73\n",
      "2017-04-07T01:00:21.860732: step 559, loss 0.518736, acc 0.73\n",
      "2017-04-07T01:00:22.135554: step 560, loss 0.509662, acc 0.71\n",
      "2017-04-07T01:00:22.411635: step 561, loss 0.457792, acc 0.77\n",
      "2017-04-07T01:00:22.682506: step 562, loss 0.540718, acc 0.7\n",
      "2017-04-07T01:00:22.957644: step 563, loss 0.523338, acc 0.72\n",
      "2017-04-07T01:00:23.234766: step 564, loss 0.501838, acc 0.76\n",
      "2017-04-07T01:00:23.516789: step 565, loss 0.443914, acc 0.77\n",
      "2017-04-07T01:00:23.784683: step 566, loss 0.506515, acc 0.77\n",
      "2017-04-07T01:00:24.060830: step 567, loss 0.503165, acc 0.76\n",
      "2017-04-07T01:00:24.333102: step 568, loss 0.554609, acc 0.72\n",
      "2017-04-07T01:00:24.607567: step 569, loss 0.475024, acc 0.76\n",
      "2017-04-07T01:00:24.880998: step 570, loss 0.568182, acc 0.78\n",
      "2017-04-07T01:00:25.161675: step 571, loss 0.525716, acc 0.74\n",
      "2017-04-07T01:00:25.430198: step 572, loss 0.498127, acc 0.76\n",
      "2017-04-07T01:00:25.704467: step 573, loss 0.528825, acc 0.76\n",
      "2017-04-07T01:00:25.981390: step 574, loss 0.460782, acc 0.78\n",
      "2017-04-07T01:00:26.256416: step 575, loss 0.540005, acc 0.74\n",
      "2017-04-07T01:00:26.523387: step 576, loss 0.492705, acc 0.791667\n",
      "2017-04-07T01:00:26.799397: step 577, loss 0.557094, acc 0.75\n",
      "2017-04-07T01:00:27.076772: step 578, loss 0.480061, acc 0.78\n",
      "2017-04-07T01:00:27.343686: step 579, loss 0.536665, acc 0.76\n",
      "2017-04-07T01:00:27.616546: step 580, loss 0.513259, acc 0.74\n",
      "2017-04-07T01:00:27.890991: step 581, loss 0.443685, acc 0.81\n",
      "2017-04-07T01:00:28.167630: step 582, loss 0.443869, acc 0.79\n",
      "2017-04-07T01:00:28.442002: step 583, loss 0.367445, acc 0.83\n",
      "2017-04-07T01:00:28.716901: step 584, loss 0.548456, acc 0.7\n",
      "2017-04-07T01:00:28.997198: step 585, loss 0.477403, acc 0.79\n",
      "2017-04-07T01:00:29.267251: step 586, loss 0.484226, acc 0.78\n",
      "2017-04-07T01:00:29.540832: step 587, loss 0.429938, acc 0.8\n",
      "2017-04-07T01:00:29.814314: step 588, loss 0.382263, acc 0.84\n",
      "2017-04-07T01:00:30.086575: step 589, loss 0.511663, acc 0.73\n",
      "2017-04-07T01:00:30.359019: step 590, loss 0.406923, acc 0.82\n",
      "2017-04-07T01:00:30.632893: step 591, loss 0.552715, acc 0.76\n",
      "2017-04-07T01:00:30.905586: step 592, loss 0.482994, acc 0.81\n",
      "2017-04-07T01:00:31.185473: step 593, loss 0.496976, acc 0.75\n",
      "2017-04-07T01:00:31.458700: step 594, loss 0.387839, acc 0.81\n",
      "2017-04-07T01:00:31.732459: step 595, loss 0.469239, acc 0.8\n",
      "2017-04-07T01:00:32.010873: step 596, loss 0.475744, acc 0.83\n",
      "2017-04-07T01:00:32.288483: step 597, loss 0.658334, acc 0.65\n",
      "2017-04-07T01:00:32.561082: step 598, loss 0.521535, acc 0.71\n",
      "2017-04-07T01:00:32.833777: step 599, loss 0.440781, acc 0.82\n",
      "2017-04-07T01:00:33.108797: step 600, loss 0.406634, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:00:33.813083: step 600, loss 0.595015, acc 0.673546\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-600\n",
      "\n",
      "2017-04-07T01:00:34.596675: step 601, loss 0.418634, acc 0.79\n",
      "2017-04-07T01:00:34.885178: step 602, loss 0.489351, acc 0.78\n",
      "2017-04-07T01:00:35.231123: step 603, loss 0.501888, acc 0.76\n",
      "2017-04-07T01:00:35.588850: step 604, loss 0.42365, acc 0.8\n",
      "2017-04-07T01:00:35.909669: step 605, loss 0.348246, acc 0.85\n",
      "2017-04-07T01:00:36.241576: step 606, loss 0.438489, acc 0.81\n",
      "2017-04-07T01:00:36.592447: step 607, loss 0.40242, acc 0.81\n",
      "2017-04-07T01:00:36.902706: step 608, loss 0.3857, acc 0.83\n",
      "2017-04-07T01:00:37.228978: step 609, loss 0.452131, acc 0.75\n",
      "2017-04-07T01:00:37.557378: step 610, loss 0.43213, acc 0.8\n",
      "2017-04-07T01:00:37.851787: step 611, loss 0.519921, acc 0.71\n",
      "2017-04-07T01:00:38.128744: step 612, loss 0.428486, acc 0.77\n",
      "2017-04-07T01:00:38.492529: step 613, loss 0.634838, acc 0.68\n",
      "2017-04-07T01:00:38.839101: step 614, loss 0.377487, acc 0.86\n",
      "2017-04-07T01:00:39.200880: step 615, loss 0.506726, acc 0.74\n",
      "2017-04-07T01:00:39.484625: step 616, loss 0.416765, acc 0.82\n",
      "2017-04-07T01:00:39.784140: step 617, loss 0.422098, acc 0.83\n",
      "2017-04-07T01:00:40.106167: step 618, loss 0.404743, acc 0.78\n",
      "2017-04-07T01:00:40.393625: step 619, loss 0.499754, acc 0.78\n",
      "2017-04-07T01:00:40.698042: step 620, loss 0.363636, acc 0.88\n",
      "2017-04-07T01:00:40.973871: step 621, loss 0.445214, acc 0.81\n",
      "2017-04-07T01:00:41.277707: step 622, loss 0.415536, acc 0.8\n",
      "2017-04-07T01:00:41.586012: step 623, loss 0.441138, acc 0.8\n",
      "2017-04-07T01:00:41.921995: step 624, loss 0.517132, acc 0.73\n",
      "2017-04-07T01:00:42.238905: step 625, loss 0.430768, acc 0.8\n",
      "2017-04-07T01:00:42.560482: step 626, loss 0.546657, acc 0.75\n",
      "2017-04-07T01:00:42.876934: step 627, loss 0.472167, acc 0.78\n",
      "2017-04-07T01:00:43.192124: step 628, loss 0.513667, acc 0.75\n",
      "2017-04-07T01:00:43.464403: step 629, loss 0.424097, acc 0.83\n",
      "2017-04-07T01:00:43.744004: step 630, loss 0.427338, acc 0.82\n",
      "2017-04-07T01:00:44.039733: step 631, loss 0.496527, acc 0.72\n",
      "2017-04-07T01:00:44.359641: step 632, loss 0.437137, acc 0.8\n",
      "2017-04-07T01:00:44.655649: step 633, loss 0.38671, acc 0.85\n",
      "2017-04-07T01:00:45.016883: step 634, loss 0.487794, acc 0.75\n",
      "2017-04-07T01:00:45.306483: step 635, loss 0.540925, acc 0.75\n",
      "2017-04-07T01:00:45.597714: step 636, loss 0.507331, acc 0.74\n",
      "2017-04-07T01:00:45.897212: step 637, loss 0.424159, acc 0.79\n",
      "2017-04-07T01:00:46.167384: step 638, loss 0.414322, acc 0.79\n",
      "2017-04-07T01:00:46.446401: step 639, loss 0.524906, acc 0.72\n",
      "2017-04-07T01:00:46.725235: step 640, loss 0.455875, acc 0.77\n",
      "2017-04-07T01:00:46.996648: step 641, loss 0.471608, acc 0.8\n",
      "2017-04-07T01:00:47.275532: step 642, loss 0.464525, acc 0.79\n",
      "2017-04-07T01:00:47.584563: step 643, loss 0.525029, acc 0.76\n",
      "2017-04-07T01:00:47.894439: step 644, loss 0.473319, acc 0.74\n",
      "2017-04-07T01:00:48.222471: step 645, loss 0.443375, acc 0.81\n",
      "2017-04-07T01:00:48.581136: step 646, loss 0.40894, acc 0.81\n",
      "2017-04-07T01:00:48.865796: step 647, loss 0.569631, acc 0.69\n",
      "2017-04-07T01:00:49.152542: step 648, loss 0.475505, acc 0.78\n",
      "2017-04-07T01:00:49.424094: step 649, loss 0.461516, acc 0.78\n",
      "2017-04-07T01:00:49.688057: step 650, loss 0.392782, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:00:50.379066: step 650, loss 0.588088, acc 0.681051\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-650\n",
      "\n",
      "2017-04-07T01:00:51.218075: step 651, loss 0.556787, acc 0.72\n",
      "2017-04-07T01:00:51.508663: step 652, loss 0.462873, acc 0.82\n",
      "2017-04-07T01:00:51.807658: step 653, loss 0.460844, acc 0.81\n",
      "2017-04-07T01:00:52.101208: step 654, loss 0.441997, acc 0.8\n",
      "2017-04-07T01:00:52.398369: step 655, loss 0.508188, acc 0.73\n",
      "2017-04-07T01:00:52.698892: step 656, loss 0.423114, acc 0.81\n",
      "2017-04-07T01:00:53.059610: step 657, loss 0.522338, acc 0.71\n",
      "2017-04-07T01:00:53.413266: step 658, loss 0.470079, acc 0.8\n",
      "2017-04-07T01:00:53.867119: step 659, loss 0.45267, acc 0.8\n",
      "2017-04-07T01:00:54.256050: step 660, loss 0.467457, acc 0.79\n",
      "2017-04-07T01:00:54.598050: step 661, loss 0.386868, acc 0.82\n",
      "2017-04-07T01:00:54.927858: step 662, loss 0.483497, acc 0.74\n",
      "2017-04-07T01:00:55.286787: step 663, loss 0.491647, acc 0.79\n",
      "2017-04-07T01:00:55.591436: step 664, loss 0.441898, acc 0.79\n",
      "2017-04-07T01:00:55.890825: step 665, loss 0.46257, acc 0.77\n",
      "2017-04-07T01:00:56.192919: step 666, loss 0.417473, acc 0.81\n",
      "2017-04-07T01:00:56.548804: step 667, loss 0.49791, acc 0.77\n",
      "2017-04-07T01:00:56.851847: step 668, loss 0.50481, acc 0.74\n",
      "2017-04-07T01:00:57.160567: step 669, loss 0.502396, acc 0.74\n",
      "2017-04-07T01:00:57.472011: step 670, loss 0.474257, acc 0.75\n",
      "2017-04-07T01:00:57.768986: step 671, loss 0.489476, acc 0.77\n",
      "2017-04-07T01:00:58.057062: step 672, loss 0.489722, acc 0.78125\n",
      "2017-04-07T01:00:58.353062: step 673, loss 0.350365, acc 0.87\n",
      "2017-04-07T01:00:58.650984: step 674, loss 0.396542, acc 0.83\n",
      "2017-04-07T01:00:58.938762: step 675, loss 0.388588, acc 0.79\n",
      "2017-04-07T01:00:59.237171: step 676, loss 0.380132, acc 0.87\n",
      "2017-04-07T01:00:59.528038: step 677, loss 0.450841, acc 0.77\n",
      "2017-04-07T01:00:59.829011: step 678, loss 0.366935, acc 0.83\n",
      "2017-04-07T01:01:00.264819: step 679, loss 0.435262, acc 0.81\n",
      "2017-04-07T01:01:00.720654: step 680, loss 0.539628, acc 0.76\n",
      "2017-04-07T01:01:01.136626: step 681, loss 0.50434, acc 0.81\n",
      "2017-04-07T01:01:01.536319: step 682, loss 0.395269, acc 0.86\n",
      "2017-04-07T01:01:01.899117: step 683, loss 0.372065, acc 0.8\n",
      "2017-04-07T01:01:02.250429: step 684, loss 0.4285, acc 0.81\n",
      "2017-04-07T01:01:02.587833: step 685, loss 0.452915, acc 0.76\n",
      "2017-04-07T01:01:02.955008: step 686, loss 0.429007, acc 0.82\n",
      "2017-04-07T01:01:03.262667: step 687, loss 0.410988, acc 0.84\n",
      "2017-04-07T01:01:03.706457: step 688, loss 0.499344, acc 0.78\n",
      "2017-04-07T01:01:04.023878: step 689, loss 0.395717, acc 0.82\n",
      "2017-04-07T01:01:04.527446: step 690, loss 0.374365, acc 0.88\n",
      "2017-04-07T01:01:04.960567: step 691, loss 0.420261, acc 0.81\n",
      "2017-04-07T01:01:05.372677: step 692, loss 0.415094, acc 0.82\n",
      "2017-04-07T01:01:05.961296: step 693, loss 0.437033, acc 0.83\n",
      "2017-04-07T01:01:06.527561: step 694, loss 0.373786, acc 0.83\n",
      "2017-04-07T01:01:07.072040: step 695, loss 0.471444, acc 0.8\n",
      "2017-04-07T01:01:07.599213: step 696, loss 0.399587, acc 0.83\n",
      "2017-04-07T01:01:08.145070: step 697, loss 0.424147, acc 0.84\n",
      "2017-04-07T01:01:08.622828: step 698, loss 0.401774, acc 0.81\n",
      "2017-04-07T01:01:09.063597: step 699, loss 0.406005, acc 0.8\n",
      "2017-04-07T01:01:09.473173: step 700, loss 0.459312, acc 0.78\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:01:11.061046: step 700, loss 0.584477, acc 0.684803\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-700\n",
      "\n",
      "2017-04-07T01:01:12.681617: step 701, loss 0.414268, acc 0.81\n",
      "2017-04-07T01:01:13.315987: step 702, loss 0.479527, acc 0.75\n",
      "2017-04-07T01:01:13.901409: step 703, loss 0.491408, acc 0.74\n",
      "2017-04-07T01:01:14.421225: step 704, loss 0.348873, acc 0.79\n",
      "2017-04-07T01:01:14.933967: step 705, loss 0.401019, acc 0.82\n",
      "2017-04-07T01:01:15.542919: step 706, loss 0.428928, acc 0.82\n",
      "2017-04-07T01:01:16.247883: step 707, loss 0.336337, acc 0.89\n",
      "2017-04-07T01:01:16.919376: step 708, loss 0.421162, acc 0.76\n",
      "2017-04-07T01:01:17.578667: step 709, loss 0.423528, acc 0.83\n",
      "2017-04-07T01:01:18.240263: step 710, loss 0.46691, acc 0.77\n",
      "2017-04-07T01:01:18.946617: step 711, loss 0.347101, acc 0.86\n",
      "2017-04-07T01:01:19.493424: step 712, loss 0.428209, acc 0.81\n",
      "2017-04-07T01:01:20.145046: step 713, loss 0.505257, acc 0.73\n",
      "2017-04-07T01:01:20.782992: step 714, loss 0.421436, acc 0.85\n",
      "2017-04-07T01:01:21.326322: step 715, loss 0.456034, acc 0.78\n",
      "2017-04-07T01:01:21.919566: step 716, loss 0.375861, acc 0.85\n",
      "2017-04-07T01:01:22.512509: step 717, loss 0.401527, acc 0.81\n",
      "2017-04-07T01:01:22.987390: step 718, loss 0.418759, acc 0.79\n",
      "2017-04-07T01:01:23.711157: step 719, loss 0.507797, acc 0.73\n",
      "2017-04-07T01:01:24.209562: step 720, loss 0.358888, acc 0.88\n",
      "2017-04-07T01:01:24.671426: step 721, loss 0.355457, acc 0.85\n",
      "2017-04-07T01:01:25.164094: step 722, loss 0.419253, acc 0.77\n",
      "2017-04-07T01:01:25.683500: step 723, loss 0.409829, acc 0.8\n",
      "2017-04-07T01:01:26.475496: step 724, loss 0.474646, acc 0.8\n",
      "2017-04-07T01:01:27.155464: step 725, loss 0.37773, acc 0.83\n",
      "2017-04-07T01:01:27.856802: step 726, loss 0.374748, acc 0.83\n",
      "2017-04-07T01:01:28.435067: step 727, loss 0.559075, acc 0.68\n",
      "2017-04-07T01:01:28.996183: step 728, loss 0.451276, acc 0.79\n",
      "2017-04-07T01:01:29.527733: step 729, loss 0.339832, acc 0.86\n",
      "2017-04-07T01:01:30.009643: step 730, loss 0.411262, acc 0.81\n",
      "2017-04-07T01:01:30.566891: step 731, loss 0.368491, acc 0.85\n",
      "2017-04-07T01:01:31.090099: step 732, loss 0.378376, acc 0.81\n",
      "2017-04-07T01:01:31.697838: step 733, loss 0.395384, acc 0.81\n",
      "2017-04-07T01:01:32.398981: step 734, loss 0.44089, acc 0.81\n",
      "2017-04-07T01:01:33.185962: step 735, loss 0.351153, acc 0.86\n",
      "2017-04-07T01:01:33.957947: step 736, loss 0.377404, acc 0.84\n",
      "2017-04-07T01:01:34.684961: step 737, loss 0.428246, acc 0.79\n",
      "2017-04-07T01:01:35.589961: step 738, loss 0.370132, acc 0.88\n",
      "2017-04-07T01:01:36.291593: step 739, loss 0.430204, acc 0.82\n",
      "2017-04-07T01:01:37.000648: step 740, loss 0.509177, acc 0.75\n",
      "2017-04-07T01:01:37.849736: step 741, loss 0.40109, acc 0.77\n",
      "2017-04-07T01:01:38.531425: step 742, loss 0.511319, acc 0.78\n",
      "2017-04-07T01:01:39.187046: step 743, loss 0.413346, acc 0.83\n",
      "2017-04-07T01:01:39.843922: step 744, loss 0.443687, acc 0.76\n",
      "2017-04-07T01:01:40.369042: step 745, loss 0.446314, acc 0.79\n",
      "2017-04-07T01:01:40.883923: step 746, loss 0.369656, acc 0.83\n",
      "2017-04-07T01:01:41.498775: step 747, loss 0.501954, acc 0.7\n",
      "2017-04-07T01:01:42.002921: step 748, loss 0.317872, acc 0.86\n",
      "2017-04-07T01:01:42.658830: step 749, loss 0.415424, acc 0.82\n",
      "2017-04-07T01:01:43.316339: step 750, loss 0.46693, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:01:45.046256: step 750, loss 0.591968, acc 0.678236\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-750\n",
      "\n",
      "2017-04-07T01:01:47.608226: step 751, loss 0.438079, acc 0.79\n",
      "2017-04-07T01:01:48.297008: step 752, loss 0.576935, acc 0.7\n",
      "2017-04-07T01:01:48.820899: step 753, loss 0.434215, acc 0.81\n",
      "2017-04-07T01:01:49.336957: step 754, loss 0.414698, acc 0.81\n",
      "2017-04-07T01:01:49.823519: step 755, loss 0.394428, acc 0.81\n",
      "2017-04-07T01:01:50.230921: step 756, loss 0.415449, acc 0.82\n",
      "2017-04-07T01:01:50.620425: step 757, loss 0.391512, acc 0.82\n",
      "2017-04-07T01:01:51.036234: step 758, loss 0.420288, acc 0.8\n",
      "2017-04-07T01:01:51.449308: step 759, loss 0.45042, acc 0.81\n",
      "2017-04-07T01:01:51.912518: step 760, loss 0.350538, acc 0.85\n",
      "2017-04-07T01:01:52.461380: step 761, loss 0.380024, acc 0.87\n",
      "2017-04-07T01:01:52.915243: step 762, loss 0.446635, acc 0.8\n",
      "2017-04-07T01:01:53.298785: step 763, loss 0.503376, acc 0.82\n",
      "2017-04-07T01:01:53.680554: step 764, loss 0.444005, acc 0.81\n",
      "2017-04-07T01:01:53.971039: step 765, loss 0.420434, acc 0.84\n",
      "2017-04-07T01:01:54.308350: step 766, loss 0.453638, acc 0.79\n",
      "2017-04-07T01:01:54.718039: step 767, loss 0.481333, acc 0.77\n",
      "2017-04-07T01:01:55.000654: step 768, loss 0.378957, acc 0.84375\n",
      "2017-04-07T01:01:55.311402: step 769, loss 0.46294, acc 0.73\n",
      "2017-04-07T01:01:55.604522: step 770, loss 0.341433, acc 0.86\n",
      "2017-04-07T01:01:55.888181: step 771, loss 0.362739, acc 0.85\n",
      "2017-04-07T01:01:56.181266: step 772, loss 0.394801, acc 0.81\n",
      "2017-04-07T01:01:56.477225: step 773, loss 0.447462, acc 0.79\n",
      "2017-04-07T01:01:56.769706: step 774, loss 0.329301, acc 0.85\n",
      "2017-04-07T01:01:57.107366: step 775, loss 0.347425, acc 0.87\n",
      "2017-04-07T01:01:57.405773: step 776, loss 0.42864, acc 0.81\n",
      "2017-04-07T01:01:57.696294: step 777, loss 0.355116, acc 0.84\n",
      "2017-04-07T01:01:57.981138: step 778, loss 0.373356, acc 0.86\n",
      "2017-04-07T01:01:58.312961: step 779, loss 0.300451, acc 0.88\n",
      "2017-04-07T01:01:58.619119: step 780, loss 0.375534, acc 0.83\n",
      "2017-04-07T01:01:58.967409: step 781, loss 0.356521, acc 0.87\n",
      "2017-04-07T01:01:59.270896: step 782, loss 0.315945, acc 0.89\n",
      "2017-04-07T01:01:59.588542: step 783, loss 0.349544, acc 0.84\n",
      "2017-04-07T01:01:59.908908: step 784, loss 0.406611, acc 0.85\n",
      "2017-04-07T01:02:00.210070: step 785, loss 0.339659, acc 0.86\n",
      "2017-04-07T01:02:00.494953: step 786, loss 0.35721, acc 0.87\n",
      "2017-04-07T01:02:00.785116: step 787, loss 0.372794, acc 0.81\n",
      "2017-04-07T01:02:01.148840: step 788, loss 0.414514, acc 0.8\n",
      "2017-04-07T01:02:01.426163: step 789, loss 0.347355, acc 0.86\n",
      "2017-04-07T01:02:01.730819: step 790, loss 0.430229, acc 0.83\n",
      "2017-04-07T01:02:02.016833: step 791, loss 0.342806, acc 0.86\n",
      "2017-04-07T01:02:02.308188: step 792, loss 0.395939, acc 0.8\n",
      "2017-04-07T01:02:02.592682: step 793, loss 0.383987, acc 0.8\n",
      "2017-04-07T01:02:02.882397: step 794, loss 0.367576, acc 0.84\n",
      "2017-04-07T01:02:03.168101: step 795, loss 0.441854, acc 0.8\n",
      "2017-04-07T01:02:03.461155: step 796, loss 0.328851, acc 0.86\n",
      "2017-04-07T01:02:03.753016: step 797, loss 0.337738, acc 0.84\n",
      "2017-04-07T01:02:04.045996: step 798, loss 0.279429, acc 0.88\n",
      "2017-04-07T01:02:04.339015: step 799, loss 0.382823, acc 0.83\n",
      "2017-04-07T01:02:04.614620: step 800, loss 0.396755, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:02:05.474471: step 800, loss 0.593849, acc 0.681051\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-800\n",
      "\n",
      "2017-04-07T01:02:06.389392: step 801, loss 0.379591, acc 0.83\n",
      "2017-04-07T01:02:06.710928: step 802, loss 0.392057, acc 0.81\n",
      "2017-04-07T01:02:07.030809: step 803, loss 0.469309, acc 0.8\n",
      "2017-04-07T01:02:07.336470: step 804, loss 0.376153, acc 0.87\n",
      "2017-04-07T01:02:07.651611: step 805, loss 0.312808, acc 0.87\n",
      "2017-04-07T01:02:08.169886: step 806, loss 0.430319, acc 0.83\n",
      "2017-04-07T01:02:08.537563: step 807, loss 0.294656, acc 0.88\n",
      "2017-04-07T01:02:08.975473: step 808, loss 0.409319, acc 0.82\n",
      "2017-04-07T01:02:09.378226: step 809, loss 0.278315, acc 0.89\n",
      "2017-04-07T01:02:09.756500: step 810, loss 0.413815, acc 0.82\n",
      "2017-04-07T01:02:10.056707: step 811, loss 0.426477, acc 0.79\n",
      "2017-04-07T01:02:10.372816: step 812, loss 0.327725, acc 0.83\n",
      "2017-04-07T01:02:10.655166: step 813, loss 0.323655, acc 0.88\n",
      "2017-04-07T01:02:10.955833: step 814, loss 0.313541, acc 0.86\n",
      "2017-04-07T01:02:11.263991: step 815, loss 0.275663, acc 0.92\n",
      "2017-04-07T01:02:11.580469: step 816, loss 0.401105, acc 0.78\n",
      "2017-04-07T01:02:11.884900: step 817, loss 0.415888, acc 0.82\n",
      "2017-04-07T01:02:12.175350: step 818, loss 0.325675, acc 0.88\n",
      "2017-04-07T01:02:12.459806: step 819, loss 0.353137, acc 0.84\n",
      "2017-04-07T01:02:12.750891: step 820, loss 0.304327, acc 0.89\n",
      "2017-04-07T01:02:13.047831: step 821, loss 0.384798, acc 0.84\n",
      "2017-04-07T01:02:13.331109: step 822, loss 0.342751, acc 0.86\n",
      "2017-04-07T01:02:13.614636: step 823, loss 0.374777, acc 0.81\n",
      "2017-04-07T01:02:13.895856: step 824, loss 0.458735, acc 0.81\n",
      "2017-04-07T01:02:14.181017: step 825, loss 0.371239, acc 0.84\n",
      "2017-04-07T01:02:14.465009: step 826, loss 0.389011, acc 0.86\n",
      "2017-04-07T01:02:14.753802: step 827, loss 0.416285, acc 0.79\n",
      "2017-04-07T01:02:15.237731: step 828, loss 0.455636, acc 0.75\n",
      "2017-04-07T01:02:15.640959: step 829, loss 0.32851, acc 0.89\n",
      "2017-04-07T01:02:15.996573: step 830, loss 0.37775, acc 0.82\n",
      "2017-04-07T01:02:16.331238: step 831, loss 0.52855, acc 0.73\n",
      "2017-04-07T01:02:16.615756: step 832, loss 0.374467, acc 0.84\n",
      "2017-04-07T01:02:16.916283: step 833, loss 0.360685, acc 0.84\n",
      "2017-04-07T01:02:17.221874: step 834, loss 0.375693, acc 0.84\n",
      "2017-04-07T01:02:17.548794: step 835, loss 0.40973, acc 0.81\n",
      "2017-04-07T01:02:17.838584: step 836, loss 0.338197, acc 0.87\n",
      "2017-04-07T01:02:18.135444: step 837, loss 0.32652, acc 0.86\n",
      "2017-04-07T01:02:18.496386: step 838, loss 0.377955, acc 0.82\n",
      "2017-04-07T01:02:18.800658: step 839, loss 0.27981, acc 0.88\n",
      "2017-04-07T01:02:19.127920: step 840, loss 0.236837, acc 0.92\n",
      "2017-04-07T01:02:19.417452: step 841, loss 0.34978, acc 0.86\n",
      "2017-04-07T01:02:19.785393: step 842, loss 0.345564, acc 0.83\n",
      "2017-04-07T01:02:20.139866: step 843, loss 0.367015, acc 0.83\n",
      "2017-04-07T01:02:20.450936: step 844, loss 0.453985, acc 0.79\n",
      "2017-04-07T01:02:20.775890: step 845, loss 0.408419, acc 0.79\n",
      "2017-04-07T01:02:21.062220: step 846, loss 0.334304, acc 0.87\n",
      "2017-04-07T01:02:21.362640: step 847, loss 0.32392, acc 0.88\n",
      "2017-04-07T01:02:21.659608: step 848, loss 0.430005, acc 0.82\n",
      "2017-04-07T01:02:21.951747: step 849, loss 0.300278, acc 0.85\n",
      "2017-04-07T01:02:22.240612: step 850, loss 0.391589, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:02:23.253339: step 850, loss 0.577415, acc 0.683865\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-850\n",
      "\n",
      "2017-04-07T01:02:24.083281: step 851, loss 0.399182, acc 0.8\n",
      "2017-04-07T01:02:24.387952: step 852, loss 0.389451, acc 0.86\n",
      "2017-04-07T01:02:24.677392: step 853, loss 0.395055, acc 0.83\n",
      "2017-04-07T01:02:24.973438: step 854, loss 0.333328, acc 0.85\n",
      "2017-04-07T01:02:25.274859: step 855, loss 0.312924, acc 0.88\n",
      "2017-04-07T01:02:25.569465: step 856, loss 0.394964, acc 0.8\n",
      "2017-04-07T01:02:25.863572: step 857, loss 0.25606, acc 0.89\n",
      "2017-04-07T01:02:26.158406: step 858, loss 0.376588, acc 0.85\n",
      "2017-04-07T01:02:26.464848: step 859, loss 0.390123, acc 0.85\n",
      "2017-04-07T01:02:26.805564: step 860, loss 0.403366, acc 0.82\n",
      "2017-04-07T01:02:27.085130: step 861, loss 0.356632, acc 0.84\n",
      "2017-04-07T01:02:27.385692: step 862, loss 0.422802, acc 0.84\n",
      "2017-04-07T01:02:27.679172: step 863, loss 0.456263, acc 0.83\n",
      "2017-04-07T01:02:27.975208: step 864, loss 0.344993, acc 0.864583\n",
      "2017-04-07T01:02:28.271331: step 865, loss 0.318817, acc 0.86\n",
      "2017-04-07T01:02:28.560908: step 866, loss 0.365475, acc 0.85\n",
      "2017-04-07T01:02:28.849471: step 867, loss 0.375579, acc 0.81\n",
      "2017-04-07T01:02:29.167529: step 868, loss 0.279397, acc 0.86\n",
      "2017-04-07T01:02:29.462885: step 869, loss 0.302772, acc 0.85\n",
      "2017-04-07T01:02:29.770696: step 870, loss 0.393142, acc 0.83\n",
      "2017-04-07T01:02:30.092728: step 871, loss 0.371661, acc 0.85\n",
      "2017-04-07T01:02:30.411800: step 872, loss 0.367614, acc 0.8\n",
      "2017-04-07T01:02:30.787859: step 873, loss 0.331172, acc 0.84\n",
      "2017-04-07T01:02:31.123996: step 874, loss 0.280382, acc 0.9\n",
      "2017-04-07T01:02:31.487813: step 875, loss 0.358263, acc 0.86\n",
      "2017-04-07T01:02:31.829269: step 876, loss 0.274902, acc 0.88\n",
      "2017-04-07T01:02:32.159121: step 877, loss 0.305412, acc 0.87\n",
      "2017-04-07T01:02:32.489572: step 878, loss 0.313692, acc 0.87\n",
      "2017-04-07T01:02:32.808692: step 879, loss 0.344514, acc 0.86\n",
      "2017-04-07T01:02:33.109645: step 880, loss 0.348625, acc 0.8\n",
      "2017-04-07T01:02:33.415799: step 881, loss 0.287588, acc 0.86\n",
      "2017-04-07T01:02:33.710287: step 882, loss 0.303054, acc 0.87\n",
      "2017-04-07T01:02:34.009543: step 883, loss 0.317505, acc 0.89\n",
      "2017-04-07T01:02:34.349516: step 884, loss 0.330285, acc 0.89\n",
      "2017-04-07T01:02:34.684198: step 885, loss 0.417895, acc 0.79\n",
      "2017-04-07T01:02:35.017444: step 886, loss 0.391965, acc 0.86\n",
      "2017-04-07T01:02:35.347853: step 887, loss 0.293018, acc 0.91\n",
      "2017-04-07T01:02:35.678968: step 888, loss 0.3382, acc 0.85\n",
      "2017-04-07T01:02:35.974661: step 889, loss 0.314898, acc 0.87\n",
      "2017-04-07T01:02:36.279060: step 890, loss 0.226246, acc 0.92\n",
      "2017-04-07T01:02:36.584328: step 891, loss 0.246443, acc 0.9\n",
      "2017-04-07T01:02:36.880655: step 892, loss 0.295333, acc 0.89\n",
      "2017-04-07T01:02:37.184507: step 893, loss 0.341717, acc 0.84\n",
      "2017-04-07T01:02:37.482305: step 894, loss 0.326026, acc 0.86\n",
      "2017-04-07T01:02:37.794021: step 895, loss 0.362433, acc 0.82\n",
      "2017-04-07T01:02:38.140712: step 896, loss 0.307258, acc 0.85\n",
      "2017-04-07T01:02:38.432154: step 897, loss 0.319347, acc 0.88\n",
      "2017-04-07T01:02:38.779786: step 898, loss 0.410196, acc 0.76\n",
      "2017-04-07T01:02:39.080594: step 899, loss 0.343822, acc 0.85\n",
      "2017-04-07T01:02:39.396937: step 900, loss 0.342179, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:02:40.180492: step 900, loss 0.575632, acc 0.690432\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-900\n",
      "\n",
      "2017-04-07T01:02:41.003367: step 901, loss 0.420821, acc 0.78\n",
      "2017-04-07T01:02:41.300217: step 902, loss 0.325136, acc 0.82\n",
      "2017-04-07T01:02:41.597376: step 903, loss 0.234115, acc 0.92\n",
      "2017-04-07T01:02:41.905906: step 904, loss 0.191004, acc 0.94\n",
      "2017-04-07T01:02:42.253768: step 905, loss 0.417769, acc 0.78\n",
      "2017-04-07T01:02:42.548559: step 906, loss 0.241698, acc 0.91\n",
      "2017-04-07T01:02:42.845557: step 907, loss 0.253044, acc 0.91\n",
      "2017-04-07T01:02:43.150702: step 908, loss 0.334581, acc 0.85\n",
      "2017-04-07T01:02:43.450167: step 909, loss 0.307947, acc 0.86\n",
      "2017-04-07T01:02:43.748159: step 910, loss 0.440464, acc 0.8\n",
      "2017-04-07T01:02:44.055905: step 911, loss 0.421949, acc 0.86\n",
      "2017-04-07T01:02:44.355587: step 912, loss 0.351874, acc 0.85\n",
      "2017-04-07T01:02:44.667591: step 913, loss 0.280966, acc 0.87\n",
      "2017-04-07T01:02:44.952647: step 914, loss 0.359419, acc 0.83\n",
      "2017-04-07T01:02:45.274813: step 915, loss 0.321979, acc 0.86\n",
      "2017-04-07T01:02:45.585284: step 916, loss 0.364421, acc 0.88\n",
      "2017-04-07T01:02:45.885934: step 917, loss 0.354073, acc 0.82\n",
      "2017-04-07T01:02:46.202274: step 918, loss 0.358178, acc 0.85\n",
      "2017-04-07T01:02:46.491028: step 919, loss 0.28638, acc 0.87\n",
      "2017-04-07T01:02:46.787140: step 920, loss 0.249382, acc 0.89\n",
      "2017-04-07T01:02:47.086122: step 921, loss 0.358876, acc 0.81\n",
      "2017-04-07T01:02:47.406212: step 922, loss 0.3112, acc 0.89\n",
      "2017-04-07T01:02:47.735956: step 923, loss 0.294138, acc 0.86\n",
      "2017-04-07T01:02:48.040675: step 924, loss 0.470076, acc 0.81\n",
      "2017-04-07T01:02:48.336471: step 925, loss 0.25643, acc 0.87\n",
      "2017-04-07T01:02:48.629140: step 926, loss 0.256183, acc 0.9\n",
      "2017-04-07T01:02:48.945556: step 927, loss 0.37549, acc 0.81\n",
      "2017-04-07T01:02:49.238410: step 928, loss 0.407838, acc 0.82\n",
      "2017-04-07T01:02:49.542670: step 929, loss 0.348834, acc 0.85\n",
      "2017-04-07T01:02:49.846149: step 930, loss 0.267336, acc 0.92\n",
      "2017-04-07T01:02:50.141574: step 931, loss 0.343388, acc 0.85\n",
      "2017-04-07T01:02:50.446394: step 932, loss 0.330206, acc 0.85\n",
      "2017-04-07T01:02:50.764982: step 933, loss 0.282692, acc 0.86\n",
      "2017-04-07T01:02:51.102130: step 934, loss 0.260697, acc 0.86\n",
      "2017-04-07T01:02:51.407331: step 935, loss 0.325366, acc 0.87\n",
      "2017-04-07T01:02:51.758572: step 936, loss 0.358443, acc 0.81\n",
      "2017-04-07T01:02:52.100309: step 937, loss 0.383294, acc 0.84\n",
      "2017-04-07T01:02:52.490374: step 938, loss 0.378836, acc 0.84\n",
      "2017-04-07T01:02:52.829210: step 939, loss 0.306586, acc 0.88\n",
      "2017-04-07T01:02:53.178314: step 940, loss 0.350802, acc 0.84\n",
      "2017-04-07T01:02:53.484258: step 941, loss 0.312076, acc 0.85\n",
      "2017-04-07T01:02:53.795140: step 942, loss 0.310336, acc 0.88\n",
      "2017-04-07T01:02:54.129688: step 943, loss 0.276955, acc 0.89\n",
      "2017-04-07T01:02:54.427666: step 944, loss 0.289787, acc 0.85\n",
      "2017-04-07T01:02:54.740835: step 945, loss 0.288503, acc 0.89\n",
      "2017-04-07T01:02:55.033215: step 946, loss 0.309648, acc 0.87\n",
      "2017-04-07T01:02:55.363197: step 947, loss 0.304292, acc 0.89\n",
      "2017-04-07T01:02:55.669498: step 948, loss 0.365401, acc 0.85\n",
      "2017-04-07T01:02:55.997933: step 949, loss 0.31214, acc 0.87\n",
      "2017-04-07T01:02:56.320716: step 950, loss 0.234951, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2017-04-07T01:02:57.137129: step 950, loss 0.579812, acc 0.705441\n",
      "\n",
      "Saved model checkpoint to /Users/sin/PycharmProjects/study/cnn-text-classification-tf-master/runs/1491494246/checkpoints/model-950\n",
      "\n",
      "2017-04-07T01:02:58.038526: step 951, loss 0.405129, acc 0.82\n",
      "2017-04-07T01:02:58.391412: step 952, loss 0.380537, acc 0.88\n",
      "2017-04-07T01:02:58.715541: step 953, loss 0.319818, acc 0.86\n",
      "2017-04-07T01:02:59.042378: step 954, loss 0.324749, acc 0.86\n",
      "2017-04-07T01:02:59.339251: step 955, loss 0.27138, acc 0.87\n",
      "2017-04-07T01:02:59.650231: step 956, loss 0.345894, acc 0.83\n",
      "2017-04-07T01:02:59.947249: step 957, loss 0.271003, acc 0.86\n",
      "2017-04-07T01:03:00.267576: step 958, loss 0.261472, acc 0.89\n",
      "2017-04-07T01:03:00.582373: step 959, loss 0.300896, acc 0.85\n",
      "2017-04-07T01:03:00.874606: step 960, loss 0.315465, acc 0.84375\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "    allow_soft_placement = FLAGS.allow_soft_placement,\n",
    "    log_device_placement = FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
    "                     num_classes=y_train.shape[1],\n",
    "                     vocab_size=len(vocab_processor.vocabulary_),\n",
    "                     embedding_size=FLAGS.embedding_dim,\n",
    "                     filter_sizes=list(map(int, FLAGS.filter_sizes.split(','))),\n",
    "                     num_filters=FLAGS.num_filters\n",
    "                     )\n",
    "        \n",
    "        # define Training procedure\n",
    "        global_step = tf.Variable(0,trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # keep track of gradient values and sparsity\n",
    "        grad_summaries = []\n",
    "        for g,v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram('{}/grad/hist'.format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar('{}/grad/sparsity'.format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "        # output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, 'runs', timestamp))\n",
    "        print('writing to {}\\n'.format(out_dir))\n",
    "        \n",
    "        # summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar('loss', cnn.loss)\n",
    "        acc_summary = tf.summary.scalar('accuracy', cnn.accuracy)\n",
    "        \n",
    "        # train summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, 'summaries', 'dev')\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # checkpoint directory\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, 'checkpoints'))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, 'model')\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep = FLAGS.num_checkpoints)\n",
    "        \n",
    "        # write voca\n",
    "        vocab_processor.save(os.path.join(out_dir, 'vocab'))\n",
    "        \n",
    "        # initialize\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "                \n",
    "        # generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs\n",
    "        )\n",
    "        # training loop\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tensorboard --logdir /PATH_TO_CODE/runs/1449760558/summaries/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
